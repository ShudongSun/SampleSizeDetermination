doParallel::registerDoParallel(cl = my.cluster)
pilot_data = cbind(x,y)
names(pilot_data)[length(names(pilot_data))] = "class"
num_class = length(table(y))
num_PC = length(colnames(x))
pilot_data_rp = pilot_data
means = matrix(0, nrow = num_class, ncol = num_PC)
covs = vector(mode = "list", length = num_class)
for(i in 1:num_class){
class_i_ids = which(pilot_data$class == names(table(pilot_data$class))[i])
pilot_i_data = pilot_data[class_i_ids,]
for(j in 1:num_PC){
pilot_data_i_j = pilot_i_data[,j]
temp = qnorm(rank(pilot_data_i_j)/(length(pilot_data_i_j) + 1)) # rank
pilot_data_i_j_rp = as.numeric(quantile(pilot_data_i_j, probs = pnorm(temp))) # percentile
pilot_data_rp[class_i_ids,j] = pilot_data_i_j_rp
# pilot_data_rp[class_i_ids,j] = pilot_data_i_j
means[i,j] = mean(pilot_data_i_j_rp)
}
covs[[i]] = cov(pilot_data[class_i_ids,1:num_PC])
}
p_nclass = as.numeric(table(y))
for(i in 1:num_class){
if(i==1){
sigma_hat = (p_nclass[i]-1)*covs[[i]]
}else{
sigma_hat = sigma_hat + (p_nclass[i]-1)*covs[[i]]
}
}
sigma_hat = sigma_hat/(sum(p_nclass)-num_class)
n_train_list = n_train_list
metrics_syn <- vector(length=length(n_train_list))
j=1
for(n_train in n_train_list){
n_test = n_test
if(print_progress_bar==TRUE){pb$tick(tokens = list(message = paste("[Training size =",n_train,"is running...]")))}
result_temp <- foreach(
k = 1:num_repeat,
.combine = 'rbind',
.packages = c('MASS','randomForest','mclust','aricode','progress')
) %dopar% {
for(i in 1:num_class){
if(i==1){
train_data = cbind(data.frame(mvrnorm(n_train, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
test_data = cbind(data.frame(mvrnorm(n_test, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
}else{
train_data = rbind(train_data,cbind(mvrnorm(n_train, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
test_data = rbind(test_data,cbind(mvrnorm(n_test, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
}
}
pred = get_predictions(train_data=train_data, test_data=test_data, model=model, func=func)
# Record error rate for one fold (0 or 1 for LOOCV)
cm = table(test_data[,ncol(test_data)], pred)
errors_temp = misclass_err(test_data[,ncol(test_data)], pred)
# ARIs_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
# AMIs_temp = AMI(test_data[,ncol(test_data)], pred)
return(errors_temp)
}
metrics_syn[j] = mean(result_temp[,"errors_temp"])
j=j+1
}
result_temp
mean(result_temp)
pilot_data = cbind(x,y)
names(pilot_data)[length(names(pilot_data))] = "class"
num_class = length(table(y))
num_PC = length(colnames(x))
pilot_data_rp = pilot_data
means = matrix(0, nrow = num_class, ncol = num_PC)
covs = vector(mode = "list", length = num_class)
for(i in 1:num_class){
class_i_ids = which(pilot_data$class == names(table(pilot_data$class))[i])
pilot_i_data = pilot_data[class_i_ids,]
for(j in 1:num_PC){
pilot_data_i_j = pilot_i_data[,j]
temp = qnorm(rank(pilot_data_i_j)/(length(pilot_data_i_j) + 1)) # rank
pilot_data_i_j_rp = as.numeric(quantile(pilot_data_i_j, probs = pnorm(temp))) # percentile
pilot_data_rp[class_i_ids,j] = pilot_data_i_j_rp
# pilot_data_rp[class_i_ids,j] = pilot_data_i_j
means[i,j] = mean(pilot_data_i_j_rp)
}
covs[[i]] = cov(pilot_data[class_i_ids,1:num_PC])
}
p_nclass = as.numeric(table(y))
for(i in 1:num_class){
if(i==1){
sigma_hat = (p_nclass[i]-1)*covs[[i]]
}else{
sigma_hat = sigma_hat + (p_nclass[i]-1)*covs[[i]]
}
}
sigma_hat = sigma_hat/(sum(p_nclass)-num_class)
n_train_list = n_train_list
metrics_syn <- vector(length=length(n_train_list))
j=1
for(n_train in n_train_list){
n_test = n_test
if(print_progress_bar==TRUE){pb$tick(tokens = list(message = paste("[Training size =",n_train,"is running...]")))}
result_temp <- foreach(
k = 1:num_repeat,
.combine = 'rbind',
.packages = c('MASS','randomForest','mclust','aricode','progress')
) %dopar% {
for(i in 1:num_class){
if(i==1){
train_data = cbind(data.frame(mvrnorm(n_train, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
test_data = cbind(data.frame(mvrnorm(n_test, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
}else{
train_data = rbind(train_data,cbind(mvrnorm(n_train, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
test_data = rbind(test_data,cbind(mvrnorm(n_test, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
}
}
pred = get_predictions(train_data=train_data, test_data=test_data, model=model, func=func)
# Record error rate for one fold (0 or 1 for LOOCV)
cm = table(test_data[,ncol(test_data)], pred)
errors_temp = misclass_err(test_data[,ncol(test_data)], pred)
# ARIs_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
# AMIs_temp = AMI(test_data[,ncol(test_data)], pred)
return(errors_temp)
}
metrics_syn[j] = mean(result_temp)
j=j+1
}
metrics_syn
misclass_err
result
if(!model %in% c( "svm", "randomforest", "tree","self")){
stop('\nmodel \'',model, '\' cannot be found')
}
if(!index %in% c("ARI","AMI","classification error","self")){
stop('\nindex \'',index, '\' cannot be found')
}
n_iter <- length(n_train_list)+1
if(print_progress_bar==TRUE){
pb <- progress_bar$new(format = ":percent [:bar] [Elapsed time: :elapsedfull] :message",
total = n_iter,
complete = "=",   # Completion bar character
incomplete = "-", # Incomplete bar character
current = ">",    # Current bar character
clear = FALSE,    # If TRUE, clears the bar when finish
width = 100,      # Width of the progress bar
show_after = 0)
pb$tick(0,tokens = list(message="[Initializing...]                 "))
}
if(is.null(n.cores)){
n.cores <- parallel::detectCores() - 1
}
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
doParallel::registerDoParallel(cl = my.cluster)
if(mode=="pilot"){
pilot_data = cbind(x,y)
names(pilot_data)[length(names(pilot_data))] = "class"
num_class = length(table(y))
num_PC = length(colnames(x))
pilot_data_rp = pilot_data
means = matrix(0, nrow = num_class, ncol = num_PC)
covs = vector(mode = "list", length = num_class)
for(i in 1:num_class){
class_i_ids = which(pilot_data$class == names(table(pilot_data$class))[i])
pilot_i_data = pilot_data[class_i_ids,]
for(j in 1:num_PC){
pilot_data_i_j = pilot_i_data[,j]
temp = qnorm(rank(pilot_data_i_j)/(length(pilot_data_i_j) + 1)) # rank
pilot_data_i_j_rp = as.numeric(quantile(pilot_data_i_j, probs = pnorm(temp))) # percentile
pilot_data_rp[class_i_ids,j] = pilot_data_i_j_rp
# pilot_data_rp[class_i_ids,j] = pilot_data_i_j
means[i,j] = mean(pilot_data_i_j_rp)
}
covs[[i]] = cov(pilot_data[class_i_ids,1:num_PC])
}
p_nclass = as.numeric(table(y))
for(i in 1:num_class){
if(i==1){
sigma_hat = (p_nclass[i]-1)*covs[[i]]
}else{
sigma_hat = sigma_hat + (p_nclass[i]-1)*covs[[i]]
}
}
sigma_hat = sigma_hat/(sum(p_nclass)-num_class)
n_train_list = n_train_list
metrics_syn <- vector(length=length(n_train_list))
j=1
for(n_train in n_train_list){
n_test = n_test
if(print_progress_bar==TRUE){pb$tick(tokens = list(message = paste("[Training size =",n_train,"is running...]")))}
result_temp <- foreach(
k = 1:num_repeat,
.combine = 'rbind',
.packages = c('MASS','randomForest','mclust','aricode','progress')
) %dopar% {
for(i in 1:num_class){
if(i==1){
train_data = cbind(data.frame(mvrnorm(n_train, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
test_data = cbind(data.frame(mvrnorm(n_test, means[i,], sigma_hat)),class = factor(i,levels=1:num_class))
}else{
train_data = rbind(train_data,cbind(mvrnorm(n_train, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
test_data = rbind(test_data,cbind(mvrnorm(n_test, means[i,], sigma_hat),class = factor(i,levels=1:num_class)))
}
}
pred = get_predictions(train_data=train_data, test_data=test_data, model=model, func=func)
# Record error rate for one fold (0 or 1 for LOOCV)
if(index=="classification error"){
metrics_temp = misclass_err(test_data[,ncol(test_data)], pred)
}
if(index=="ARI"){
metrics_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
}
if(index=="AMI"){
metrics_temp = AMI(test_data[,ncol(test_data)], pred)
}
# ARIs_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
# AMIs_temp = AMI(test_data[,ncol(test_data)], pred)
return(metrics_temp)
}
metrics_syn[j] = mean(result_temp)
j=j+1
}
result = metrics_syn
}else if(mode=="true"){
if(is.null(test_x) | is.null(test_y)){
stop('\nYou must input test_x and test_y parameters when using this mode.')
}
train_data_input = cbind(x,y)
names(train_data_input)[length(names(train_data_input))] = "class"
num_class_train = length(table(y))
num_PC_train = length(colnames(x))
test_data_input = cbind(test_x, test_y)
names(test_data_input)[length(names(test_data_input))] = "class"
num_class_test = length(table(test_y))
num_PC_test = length(colnames(test_x))
num_class = num_class_train
if(num_PC_train != num_PC_test){
stop('\n# of PCs in training data must be matched with # of PCs in test data.')
}
n_train_list = n_train_list
metrics_true <- vector(length=length(n_train_list))
j=1
for(n_train in n_train_list){
# print(j)
n_test = n_test
if(print_progress_bar==TRUE){pb$tick(tokens = list(message = paste("[Training size =",n_train,"is running...]")))}
result_temp <- foreach(
k = 1:num_repeat,
.combine = 'rbind',
.packages = c('MASS','randomForest','mclust','aricode','progress')
) %dopar% {
# do here
for(i in 1:num_class){
class_i_ids_train = which(train_data_input$class == names(table(train_data_input$class))[i])
if(length(class_i_ids_train)>=n_train){
train_i_ids = sample(class_i_ids_train, n_train)
train_i_data = train_data_input[train_i_ids,]
}else{
train_i_ids = sample(class_i_ids_train,n_train,replace=TRUE)
train_i_data = train_data_input[train_i_ids,]
}
class_i_ids_test = which(test_data_input$class == names(table(test_data_input$class))[i])
if(length(class_i_ids_test)>=n_test){
test_i_ids = sample(class_i_ids_test, n_test)
test_i_data = test_data_input[test_i_ids,]
}else{
test_i_ids = sample(class_i_ids_test,n_test,replace=TRUE)
test_i_data = test_data_input[test_i_ids,]
}
if(i == 1){
train_data = train_i_data
test_data = test_i_data
}else{
train_data = rbind(train_data, train_i_data)
test_data = rbind(test_data, test_i_data)
}
}
# table(train_data$class)
# table(test_data$class)
pred = get_predictions(train_data=train_data, test_data=test_data, model=model, func=func)
# Record error rate for one fold (0 or 1 for LOOCV)
if(index=="classification error"){
metrics_temp = misclass_err(test_data[,ncol(test_data)], pred)
}
if(index=="ARI"){
metrics_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
}
if(index=="AMI"){
metrics_temp = AMI(test_data[,ncol(test_data)], pred)
}
# ARIs_temp = adjustedRandIndex(test_data[,ncol(test_data)], pred)
# AMIs_temp = AMI(test_data[,ncol(test_data)], pred)
return(metrics_temp)
}
metrics_true[j] = mean(result_temp)
j=j+1
}
result = metrics_true
}
parallel::stopCluster(cl = my.cluster)
if(mode=="pilot"){results_calculated = metrics_syn}
if(mode=="true"){results_calculated = metrics_true}
plot_fit_ipl(results_calculated, n_train_list, mode, index, model)
# if(index %in% c("ARI","AMI")){
#   result = findPC(sdev = -pred_index_syn,number = c(length(nn)),method = 'all',figure = T)
# }else if(index == "classification error"){
#   result = findPC(sdev = pred_index_syn,number = c(length(nn)),method = 'all',figure = T)
# }
if(print_progress_bar==TRUE){pb$tick(tokens = list(message="[Finished]                         "))}
result
index
library(SSD)
?ssd
pilot_data <- read.csv(system.file("extdata", "data_pbmc68k_pilot_18pc.csv", package = "SSD"),row.names=1)
x_pilot = pilot_data[,-length(pilot_data)]
y_pilot = pilot_data[,length(pilot_data)]
table(pilot_data$phenoid)
result_pilot = ssd(x_pilot, y_pilot, n_train_list = seq(from=30,to=210,by=30))
result_pilot = ssd(x_pilot, y_pilot, n_train_list = seq(from=30,to=210,by=30), index = "classification error")
train_data <- read.csv(system.file("extdata", "data_pbmc68k_train_23pc.csv", package = "SSD"),row.names=1)
test_data <- read.csv(system.file("extdata", "data_pbmc68k_test_23pc.csv", package = "SSD"),row.names=1)
x_true_train = train_data[,-length(train_data)]
y_true_train = train_data[,length(train_data)]
table(train_data$phenoid)
x_true_test = test_data[,-length(test_data)]
y_true_test = test_data[,length(test_data)]
table(test_data$phenoid)
result_true = ssd(x=x_true_train, y=y_true_train, mode="true", test_x=x_true_test, test_y=y_true_test)
result_true = ssd(x=x_true_train, y=y_true_train, mode="true", test_x=x_true_test, test_y=y_true_test, n_train_list = seq(from=30,to=210,by=30))
?adjustedRandIndex
?AMI
library(SSD)
?ssd
library(SSD)
?ssd
library(SSD)
?ssd
library(SSD)
?ssd
#' @param x A data frame or a matrix of predictors to be fitted. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data; When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param y A response vector. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data. When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param model base classification model.
#' \itemize{
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#' @description build the model and determine the sample size based on elbow method of ARI and AMI.
#'
#' @param x A data frame or a matrix of predictors to be fitted. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data; When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param y A response vector. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data. When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param model base classification model.
#' \itemize{
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#' @param x A data frame or a matrix of predictors to be fitted. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data; When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param y A response vector. When using "pilot" mode, you need to use this parameter to input the predictor variable of pilot data. When using "true" mode, you need to use this parameter to input the predictor variable of training data.
#' @param model base classification model.
#' \itemize{
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#' @param model base classification model.
#' \itemize{
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#'
#' \preformatted{library(e1071)
#' \item svm: Support Vector Machines. \code{\link[e1071]{svm}} in \code{e1071} package
#' \item randomforest: Random Forest. \code{\link[randomForest]{randomForest}} in \code{randomForest} package
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#'
#' \preformatted{library(e1071)
#'
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#' \item tree: Classificatin Tree. \code{\link[tree]{tree}} in \code{tree} package
#' \item self: You can use your self-defined function. You need to pass your self-defined function via the "func" parameter.
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#'
#' \preformatted{library(e1071)
#'
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#'     train_data = data.frame(train_data_x, as.factor(train_data_y))
#'     names(train_data)[length(train_data)] = "class"
#' }
#'
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#'
#' \preformatted{library(e1071)
#'
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#'     train_data = data.frame(train_data_x, as.factor(train_data_y))
#'     names(train_data)[length(train_data)] = "class"
#'     fit_svm<-svm(class~.,data=train_data,probability=TRUE)
#'     pred <- predict(fit_svm, test_data_x)
#' @param func If you set "model" to "self", you have to pass your self-defined model function. This function should be able to take *train_data_x* and *train_data_y* as the first two inputs to train the model and then take *test_data_x* as the third input and return the predicted scores of *test_data_x* data. For example,
#'
#' \preformatted{library(e1071)
#'
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#'     train_data = data.frame(train_data_x, as.factor(train_data_y))
#'     names(train_data)[length(train_data)] = "class"
#'     fit_svm<-svm(class~.,data=train_data,probability=TRUE)
#'     pred <- predict(fit_svm, test_data_x)
#'     return(pred)
#' }
#' \preformatted{library(e1071)
#'
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#'     train_data = data.frame(train_data_x, as.factor(train_data_y))
#'     names(train_data)[length(train_data)] = "class"
#'     fit_svm<-svm(class~.,data=train_data,probability=TRUE)
#'     pred <- predict(fit_svm, test_data_x)
#'     return(pred)
#' }
#'
#' elbow = ssd(x_pilot, y_pilot, model="self", func=predict_model)
#' predict_model <- function(train_data_x, train_data_y, test_data_x){
#'     train_data = data.frame(train_data_x, as.factor(train_data_y))
#'     names(train_data)[length(train_data)] = "class"
#'     fit_svm<-svm(class~.,data=train_data,probability=TRUE)
#'     pred <- predict(fit_svm, test_data_x)
#'     return(pred)
#' }
#'
#' elbow = ssd(x_pilot, y_pilot, model="self", func=predict_model)
#' }
#'
library(SSD)
?ssd
remove.packages("SSD")
library(SSD)
?ssd
SSD
require(SSD)
??SSD
??ssd
?ssd
library(SSD)
library(SSD)
library(SSD)
?ssd
devtools::install()
devtools::install()
library(SSD)
?ssd
library(SSD)
?ssd
devtools::install()
?ssd
require(SSD)
?ssd
?ssd
detach(package:cli, unload=TRUE)
detach(cli)
library(SSD)
?ssd
library(SSD)
?ssd
library(SSD)
library(SSD)
?ssd
roxygen2::roxygenize(package.dir = ".")
?ssd
devtools::check()
classification_accuracy <- function(prediction, true_label){
correct <- sum(true_label == prediction)
acc <- correct/length(prediction)
return(acc)
}
pilot_data <- read.csv(system.file("extdata", "data_pbmc68k_pilot_18pc.csv", package = "SSD"),row.names=1)
x_pilot = pilot_data[,-length(pilot_data)]
y_pilot = pilot_data[,length(pilot_data)]
table(pilot_data$phenoid)
result_pilot = ssd(x_pilot, y_pilot)
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30))
library(SSD)
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30))
devtools::check()
a=NULL
a==NULL
is.null(a)
library(SSD)
roxygen2::roxygenize(package.dir = ".")
?ssd
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30))
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30), metric_name = "Accuracy")
library(SSD)
roxygen2::roxygenize(package.dir = ".")
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30), metric_name = "Accuracy")
library(SSD)
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30), metric_name = "Accuracy")
result_pilot = ssd(x_pilot, y_pilot, index="self", metric_func=classification_accuracy, n_train_list = seq(from=30,to=210,by=30))
